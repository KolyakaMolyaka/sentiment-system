classifier: logistic-regression
max_words: 20
metrics:
  accuracy: 0.87
  confusion_matrix:
  - - 87
    - 1
  - - 12
    - 0
  precision: 0.8787878787878788
  recall: 0.9886363636363636
model_title: my_model3
tokenizer_description: Токенизатор реализуется функцией split в Python применённой
  к строке с текстом. Это означает, что токены выделяются путём разделения строки
  пробельным символом.
tokenizer_type: default-whitespace-tokenizer
used_default_stop_words: true
vectorization_description: Векторизация текста при помощи алгоритма "Мешок слов".
  Вектор содержит столько элементов, сколько анализируемых слов. Каждый элемент вектора
  соответствует определенному слову, а значение равно количеству раз, сколько слово
  встречается в тексте.
vectorization_type: bag-of-words
