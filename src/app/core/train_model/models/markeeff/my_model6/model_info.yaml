classifier: logistic-regression
max_words: 400
metrics:
  accuracy: 0.88
  confusion_matrix:
  - - 88
    - 0
  - - 12
    - 0
  precision: 0.88
  recall: 1.0
model_title: my_model6
tokenizer_description: Токенизатор доступен благодаря библиотеке NLTK и функции nltk.tokenize.word_tokenize(),
  с помощью которой получаются токены. Функция возвращает слоги из одного слова, а
  одно слово может содержать один или больше слогов.
tokenizer_type: nltk-tokenizer
used_default_stop_words: true
vectorization_description: Векторизация текста при помощи алгоритма "Мешок слов".
  Вектор содержит столько элементов, сколько анализируемых слов. Каждый элемент вектора
  соответствует определенному слову, а значение равно количеству раз, сколько слово
  встречается в тексте.
vectorization_type: bag-of-words
